name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Apply or Destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb
  DOMAIN: saharbittman.net

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: >
      ${{
        github.event_name == 'workflow_dispatch' ||
        (
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.name == 'CI Build & Test' &&
          github.event.workflow_run.conclusion == 'success'
        )
      }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      # ========================================
      # üîç Trivy Security Scan - ArgoCD Manifests Only
      # ========================================
      
      - name: üîç Trivy - Scan ArgoCD Kubernetes Manifests
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        continue-on-error: true
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'config'
          scan-ref: 'ArgoCD/'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          trivyignores: '.trivyignore'

      # Update kubeconfig
      - name: Update kubeconfig for EKS
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      # Configure aws-auth
      - name: Configure aws-auth
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      # Install ArgoCD
      - name: Install ArgoCD
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"

          echo "‚è≥ Waiting for argocd-server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

          # Get the LoadBalancer DNS
          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "üåê ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"

          # Get initial admin password
          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "üîë ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"


       # Deploy apps
      - name: Get VPC and Deploy Apps
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      # Smoke test
      - name: Smoke Test
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          URL="https://www.saharbittman.com/health"

          for i in {1..200}; do
            response=$(curl -k -s "$URL" || true)

            if [ "${response,,}" = "ok" ]; then
              echo "‚úÖ Health check passed! Response: $response"
              exit 0
            fi

            echo "‚è≥ Attempt $i/50 - Response: $response"
            sleep 10
          done

          echo "‚ùå Smoke test failed - health endpoint did not return 'ok' after 50 attempts"
          exit 1



      # # Push image to second AWS account only if CD succeeded

      # - name: Start ECR replicate to second account
      #   if: ${{ success() }}
      #   run: echo "üöÄ Starting image replication to second ECR..."

      # # Login to SOURCE ECR (first account)
      # - name: Login to SOURCE ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Pull image from SOURCE ECR
      # - name: Pull image from SOURCE ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     TAG: latest
      #   run: |
      #     echo "üì• Pulling image: $SRC_ECR:$TAG"
      #     docker pull "$SRC_ECR:$TAG"

      # # Assume role in SECOND ACCOUNT
      # - name: Configure AWS (SECOND ACCOUNT)
      #   if: ${{ success() }}
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME_SECOND_ACCOUNT }}
      #     aws-region: ${{ env.AWS_REGION }}
      #     role-session-name: github-actions-repl-second

      # # Login to SECOND ACCOUNT ECR
      # - name: Login to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Tag and Push to SECOND ACCOUNT ECR
      # - name: Tag & Push to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     DST_ECR: ${{ secrets.ECR_NAME_SECOND }}
      #     TAG: latest
      #   run: |
      #     echo "üè∑ Tagging: $SRC_ECR:$TAG ‚Üí $DST_ECR:$TAG"
      #     docker tag "$SRC_ECR:$TAG" "$DST_ECR:$TAG"

      #     echo "üì§ Pushing image to SECOND ACCOUNT:"
      #     docker push "$DST_ECR:$TAG"

      #     echo "‚úÖ Replication done!"


       # ========== DESTROY ==========
      - name: Destroy Everything
        if: github.event.inputs.action == 'destroy' || failure()
        run: |
          
            echo "üßπ Starting COMPLETE cleanup (ArgoCD + AWS + Helm)..."
            echo ""
            
            # ========================================
            # PART 1: ArgoCD Resources Cleanup
            # ========================================
            
            echo "üìã Part 1: ArgoCD Resources Cleanup"
            echo ""
            
            # Check what exists
            echo "Current state:"
            echo "  Applications: $(kubectl get applications -n argocd --no-headers 2>/dev/null | wc -l)"
            echo "  ApplicationSets: $(kubectl get applicationset -n argocd --no-headers 2>/dev/null | wc -l)"
            echo "  Ingress: $(kubectl get ingress -A --no-headers 2>/dev/null | wc -l)"
            echo "  TargetGroupBindings: $(kubectl get targetgroupbindings -A --no-headers 2>/dev/null | wc -l)"
            echo ""
            
            # Delete Applications
            echo "üóëÔ∏è Step 1: Deleting ArgoCD Applications..."
            kubectl delete applications --all -n argocd --wait=false 2>/dev/null || echo "No applications found"
            
            # Delete ApplicationSets
            echo "üóëÔ∏è Step 2: Deleting ArgoCD ApplicationSets..."
            kubectl delete applicationset --all -n argocd --wait=false 2>/dev/null || echo "No applicationsets found"
            
            # Wait
            echo "‚è≥ Waiting 10s for resources to start deleting..."
            sleep 10
            
            # Force cleanup stuck resources
            echo "üîß Step 3: Removing finalizers from stuck resources..."
            
            kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
            
            kubectl get applicationset.argoproj.io -n argocd -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
            
            kubectl get ingress -A -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
            
            kubectl get targetgroupbindings -A -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
            
            echo "‚è≥ Waiting 5s for cleanup to complete..."
            sleep 5
            
            # Verify K8s cleanup
            echo ""
            echo "K8s cleanup verification:"
            echo "  Applications: $(kubectl get applications -n argocd --no-headers 2>/dev/null | wc -l)"
            echo "  ApplicationSets: $(kubectl get applicationset -n argocd --no-headers 2>/dev/null | wc -l)"
            echo "  Ingress: $(kubectl get ingress -A --no-headers 2>/dev/null | wc -l)"
            echo "  TargetGroupBindings: $(kubectl get targetgroupbindings -A --no-headers 2>/dev/null | wc -l)"
            
            echo ""
            echo "‚úÖ ArgoCD resources cleanup completed"
            echo ""
            
            # ========================================
            # PART 2: Uninstall ArgoCD
            # ========================================
            
            echo "üìã Part 2: Uninstall ArgoCD"
            echo ""
            echo "üóëÔ∏è Step 4: Uninstalling ArgoCD via Helm..."
            helm uninstall argocd -n argocd --timeout=2m 2>/dev/null || echo "ArgoCD already uninstalled or not found"
            
            echo "‚úÖ ArgoCD uninstalled"
            echo ""
            
            # ========================================
            # PART 3: AWS Resources Cleanup
            # ========================================
            
            echo "üìã Part 3: AWS Resources Cleanup"
            echo ""
            
            # Step 1: Delete ALB
            echo "üóëÔ∏è Step 5: Deleting ALB..."
            ALB_ARN=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'eksdemo-flask-alb')].LoadBalancerArn" \
              --output text 2>/dev/null)
            
            if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
              echo "Found ALB: $ALB_ARN"
              aws elbv2 delete-load-balancer --load-balancer-arn "$ALB_ARN"
              echo "‚úÖ ALB deletion initiated, waiting 60s..."
              sleep 60
            else
              echo "‚úÖ No ALB found"
            fi
            
            # Step 2: Delete Target Groups
            echo ""
            echo "üóëÔ∏è Step 6: Deleting Target Groups..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?contains(TargetGroupName, 'k8s-')].TargetGroupArn" \
              --output text 2>/dev/null)
            
            if [ -n "$TG_ARNS" ]; then
              for TG in $TG_ARNS; do
                echo "Deleting: $TG"
                aws elbv2 delete-target-group --target-group-arn "$TG" 2>/dev/null || echo "Failed to delete $TG"
              done
              echo "‚úÖ Target Groups deleted, waiting 30s..."
              sleep 30
            else
              echo "‚úÖ No Target Groups found"
            fi
            
            # Step 3: Delete Security Groups
            echo ""
            echo "üóëÔ∏è Step 7: Deleting Security Groups..."
            SG_IDS=$(aws ec2 describe-security-groups \
              --filters "Name=tag:elbv2.k8s.aws/cluster,Values=eksdemo-cluster" \
              --query "SecurityGroups[*].GroupId" \
              --output text 2>/dev/null)
            
            if [ -n "$SG_IDS" ]; then
              for SG in $SG_IDS; do
                echo "Attempting: $SG"
                for i in {1..5}; do
                  aws ec2 delete-security-group --group-id "$SG" 2>/dev/null && {
                    echo "‚úÖ Deleted: $SG"
                    break
                  } || {
                    if [ $i -eq 5 ]; then
                      echo "‚ö†Ô∏è Failed after 5 attempts: $SG (may have dependencies)"
                    else
                      echo "   Retry $i/5 in 10s..."
                      sleep 10
                    fi
                  }
                done
              done
            else
              echo "‚úÖ No Security Groups found"
            fi
            
            # ========================================
            # FINAL VERIFICATION
            # ========================================
            
            echo ""
            echo "=========================================="
            echo "üîç FINAL VERIFICATION"
            echo "=========================================="
            echo ""
            
            # K8s resources
            echo "üì¶ Kubernetes Resources:"
            K8S_APPS=$(kubectl get applications -n argocd --no-headers 2>/dev/null | wc -l)
            K8S_APPSETS=$(kubectl get applicationset -n argocd --no-headers 2>/dev/null | wc -l)
            K8S_INGRESS=$(kubectl get ingress -A --no-headers 2>/dev/null | wc -l)
            K8S_TGB=$(kubectl get targetgroupbindings -A --no-headers 2>/dev/null | wc -l)
            
            echo "  Applications: $K8S_APPS"
            echo "  ApplicationSets: $K8S_APPSETS"
            echo "  Ingress: $K8S_INGRESS"
            echo "  TargetGroupBindings: $K8S_TGB"
            
            # Check ArgoCD namespace
            ARGOCD_PODS=$(kubectl get pods -n argocd --no-headers 2>/dev/null | wc -l)
            echo "  ArgoCD Pods: $ARGOCD_PODS"
            
            # AWS resources
            echo ""
            echo "‚òÅÔ∏è AWS Resources:"
            ALB_COUNT=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'eksdemo-flask-alb')] | length(@)" \
              --output text 2>/dev/null || echo "0")
            TG_COUNT=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?contains(TargetGroupName, 'k8s-')] | length(@)" \
              --output text 2>/dev/null || echo "0")
            SG_COUNT=$(aws ec2 describe-security-groups \
              --filters "Name=tag:elbv2.k8s.aws/cluster,Values=eksdemo-cluster" \
              --query "SecurityGroups | length(@)" \
              --output text 2>/dev/null || echo "0")
            
            echo "  ALBs: $ALB_COUNT"
            echo "  Target Groups: $TG_COUNT"
            echo "  Security Groups: $SG_COUNT"
            
            echo ""
            K8S_TOTAL=$((K8S_APPS + K8S_APPSETS + K8S_INGRESS + K8S_TGB + ARGOCD_PODS))
            AWS_TOTAL=$((ALB_COUNT + TG_COUNT + SG_COUNT))
            GRAND_TOTAL=$((K8S_TOTAL + AWS_TOTAL))
            
            if [ "$GRAND_TOTAL" -eq "0" ]; then
              echo "‚úÖ‚úÖ‚úÖ PERFECT! All resources cleaned up successfully! ‚úÖ‚úÖ‚úÖ"
            else
              echo "‚ö†Ô∏è $GRAND_TOTAL resource(s) still remaining"
              echo "   Kubernetes: $K8S_TOTAL"
              echo "   AWS: $AWS_TOTAL"
              echo ""
              echo "You may need to check AWS Console or run: kubectl delete namespace argocd --force"
            fi
            
            echo ""
            echo "üéâ Complete cleanup script finished!"
            echo "=========================================="
          

  destroy-bootstrap:
    needs: deploy
    if: github.event.inputs.action == 'destroy' && success()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infra
        run: |
          cd infra
          terraform destroy -auto-approve