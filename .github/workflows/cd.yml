name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Apply or Destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb
  DOMAIN: saharbittman.net

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: >
      ${{
        github.event_name == 'workflow_dispatch' ||
        (
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.name == 'CI Build & Test' &&
          github.event.workflow_run.conclusion == 'success'
        )
      }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      # ========================================
      # üîç Trivy Security Scan - ArgoCD Manifests Only
      # ========================================
      
      - name: üîç Trivy - Scan ArgoCD Kubernetes Manifests
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        continue-on-error: true
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'config'
          scan-ref: 'ArgoCD/'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          trivyignores: '.trivyignore'

      # Update kubeconfig
      - name: Update kubeconfig for EKS
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      # Configure aws-auth
      - name: Configure aws-auth
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      # Install ArgoCD
      - name: Install ArgoCD
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"

          echo "‚è≥ Waiting for argocd-server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

          # Get the LoadBalancer DNS
          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "üåê ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"

          # Get initial admin password
          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "üîë ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"


       # Deploy apps
      - name: Get VPC and Deploy Apps
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      # Smoke test
      - name: Smoke Test
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          URL="https://www.saharbittman.com/health"

          for i in {1..200}; do
            response=$(curl -k -s "$URL" || true)

            if [ "${response,,}" = "ok" ]; then
              echo "‚úÖ Health check passed! Response: $response"
              exit 0
            fi

            echo "‚è≥ Attempt $i/50 - Response: $response"
            sleep 10
          done

          echo "‚ùå Smoke test failed - health endpoint did not return 'ok' after 50 attempts"
          exit 1



      # # Push image to second AWS account only if CD succeeded

      # - name: Start ECR replicate to second account
      #   if: ${{ success() }}
      #   run: echo "üöÄ Starting image replication to second ECR..."

      # # Login to SOURCE ECR (first account)
      # - name: Login to SOURCE ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Pull image from SOURCE ECR
      # - name: Pull image from SOURCE ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     TAG: latest
      #   run: |
      #     echo "üì• Pulling image: $SRC_ECR:$TAG"
      #     docker pull "$SRC_ECR:$TAG"

      # # Assume role in SECOND ACCOUNT
      # - name: Configure AWS (SECOND ACCOUNT)
      #   if: ${{ success() }}
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME_SECOND_ACCOUNT }}
      #     aws-region: ${{ env.AWS_REGION }}
      #     role-session-name: github-actions-repl-second

      # # Login to SECOND ACCOUNT ECR
      # - name: Login to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Tag and Push to SECOND ACCOUNT ECR
      # - name: Tag & Push to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     DST_ECR: ${{ secrets.ECR_NAME_SECOND }}
      #     TAG: latest
      #   run: |
      #     echo "üè∑ Tagging: $SRC_ECR:$TAG ‚Üí $DST_ECR:$TAG"
      #     docker tag "$SRC_ECR:$TAG" "$DST_ECR:$TAG"

      #     echo "üì§ Pushing image to SECOND ACCOUNT:"
      #     docker push "$DST_ECR:$TAG"

      #     echo "‚úÖ Replication done!"


       # ========== DESTROY ==========
      - name: Destroy Everything
        if: github.event.inputs.action == 'destroy' || failure()
        run: |
          echo "üóëÔ∏è Starting cleanup..."
          aws eks update-kubeconfig --region "${{ env.AWS_REGION }}" --name "${{ env.CLUSTER_NAME }}" || true
          
          # ========================================
          # CRITICAL: Delete through K8s, NOT AWS CLI!
          # Let the controllers handle AWS cleanup
          # ========================================
          
          # ========================================
          # STEP 1: Delete Ingress resources (ALB Controller will cleanup ALB/TG/SG)
          # ========================================
          echo "üóëÔ∏è Step 1: Deleting ingresses through kubectl..."
          echo "‚ö†Ô∏è Keeping finalizers - letting ALB Controller cleanup AWS resources"
          
          kubectl delete ingress --all -A --wait=true --timeout=5m || {
            echo "‚ö†Ô∏è Ingress deletion timed out or failed, checking status..."
            kubectl get ingress -A
          }
          
          # Wait a bit for controller to process
          echo "‚è≥ Waiting 30s for ALB Controller to process deletion..."
          sleep 30
          
          # Check if any ingress stuck in Terminating
          STUCK_INGRESS=$(kubectl get ingress -A --field-selector metadata.deletionTimestamp!='' --no-headers 2>/dev/null | wc -l)
          if [ "$STUCK_INGRESS" -gt "0" ]; then
            echo "‚ö†Ô∏è Found $STUCK_INGRESS ingress(es) stuck in Terminating state"
            kubectl get ingress -A --field-selector metadata.deletionTimestamp!=''
            
            echo "üîß Removing finalizers from stuck ingresses..."
            kubectl get ingress -A -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          else
            echo "‚úÖ All ingresses deleted cleanly"
          fi
          
          # ========================================
          # STEP 2: Delete ArgoCD Applications (External DNS will cleanup Route53)
          # ========================================
          echo "üóëÔ∏è Step 2: Deleting ArgoCD Applications..."
          echo "‚ö†Ô∏è Keeping finalizers - letting External DNS cleanup Route53"
          
          kubectl delete applications.argoproj.io --all -n argocd --wait=true --timeout=3m || {
            echo "‚ö†Ô∏è Applications deletion timed out or failed"
          }
          
          # Wait for External DNS to cleanup
          echo "‚è≥ Waiting 30s for External DNS to process deletion..."
          sleep 30
          
          # Check for stuck applications
          STUCK_APPS=$(kubectl get applications.argoproj.io -n argocd --field-selector metadata.deletionTimestamp!='' --no-headers 2>/dev/null | wc -l)
          if [ "$STUCK_APPS" -gt "0" ]; then
            echo "‚ö†Ô∏è Found $STUCK_APPS application(s) stuck in Terminating state"
            kubectl get applications.argoproj.io -n argocd --field-selector metadata.deletionTimestamp!=''
            
            echo "üîß Removing finalizers from stuck applications..."
            kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          else
            echo "‚úÖ All applications deleted cleanly"
          fi
          
          # ========================================
          # STEP 3: Delete ApplicationSets (this removes ALB Controller & External DNS)
          # ========================================
          echo "üóëÔ∏è Step 3: Deleting ArgoCD ApplicationSets..."
          
          kubectl delete applicationset.argoproj.io --all -n argocd --wait=true --timeout=3m || {
            echo "‚ö†Ô∏è ApplicationSets deletion timed out or failed"
          }
          
          # Check for stuck applicationsets
          STUCK_APPSETS=$(kubectl get applicationset.argoproj.io -n argocd --field-selector metadata.deletionTimestamp!='' --no-headers 2>/dev/null | wc -l)
          if [ "$STUCK_APPSETS" -gt "0" ]; then
            echo "‚ö†Ô∏è Found $STUCK_APPSETS applicationset(s) stuck in Terminating state"
            kubectl get applicationset.argoproj.io -n argocd --field-selector metadata.deletionTimestamp!=''
            
            echo "üîß Removing finalizers from stuck applicationsets..."
            kubectl get applicationset.argoproj.io -n argocd -o name 2>/dev/null | \
              xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          else
            echo "‚úÖ All applicationsets deleted cleanly"
          fi
          
          # ========================================
          # STEP 4: Uninstall ArgoCD
          # ========================================
          echo "üóëÔ∏è Step 4: Uninstalling ArgoCD..."
          helm uninstall argocd -n argocd --timeout=2m || true
          
          # ========================================
          # STEP 5: Verify cleanup (using AWS CLI only for verification)
          # ========================================
          echo "üîç Step 5: Verifying cleanup (this is just a check, not deletion)..."
          
          echo ""
          echo "üìã Checking for orphaned ALBs..."
          ALB_COUNT=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.FLASK_ALB }}')] | length(@)" \
            --output text 2>/dev/null || echo "0")
          
          if [ "$ALB_COUNT" -gt "0" ]; then
            echo "‚ùå Found $ALB_COUNT orphaned ALB(s):"
            aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.FLASK_ALB }}')].[LoadBalancerName,LoadBalancerArn]" \
              --output table
          else
            echo "‚úÖ No orphaned ALBs"
          fi
          
          echo ""
          echo "üìã Checking for orphaned Target Groups..."
          TG_COUNT=$(aws elbv2 describe-target-groups \
            --query "TargetGroups[?contains(TargetGroupName, 'k8s-')] | length(@)" \
            --output text 2>/dev/null || echo "0")
          
          if [ "$TG_COUNT" -gt "0" ]; then
            echo "‚ùå Found $TG_COUNT orphaned Target Group(s):"
            aws elbv2 describe-target-groups \
              --query "TargetGroups[?contains(TargetGroupName, 'k8s-')].[TargetGroupName,TargetGroupArn]" \
              --output table
          else
            echo "‚úÖ No orphaned Target Groups"
          fi
          
          echo ""
          echo "üìã Checking for orphaned Security Groups..."
          SG_COUNT=$(aws ec2 describe-security-groups \
            --filters "Name=tag:elbv2.k8s.aws/cluster,Values=${{ env.CLUSTER_NAME }}" \
            --query "SecurityGroups | length(@)" \
            --output text 2>/dev/null || echo "0")
          
          if [ "$SG_COUNT" -gt "0" ]; then
            echo "‚ùå Found $SG_COUNT orphaned Security Group(s):"
            aws ec2 describe-security-groups \
              --filters "Name=tag:elbv2.k8s.aws/cluster,Values=${{ env.CLUSTER_NAME }}" \
              --query "SecurityGroups[*].[GroupName,GroupId]" \
              --output table
          else
            echo "‚úÖ No orphaned Security Groups"
          fi
          
          echo ""
          echo "üìã Checking for orphaned Route53 records..."
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name \
            --dns-name "${{ env.DOMAIN }}" \
            --query "HostedZones[0].Id" \
            --output text 2>/dev/null | cut -d'/' -f3)
          
          if [ -n "$HOSTED_ZONE_ID" ] && [ "$HOSTED_ZONE_ID" != "None" ]; then
            DNS_COUNT=$(aws route53 list-resource-record-sets \
              --hosted-zone-id "$HOSTED_ZONE_ID" \
              --query "ResourceRecordSets[?Name=='www.${{ env.DOMAIN }}.'] | length(@)" \
              --output text 2>/dev/null || echo "0")
            
            if [ "$DNS_COUNT" -gt "0" ]; then
              echo "‚ùå Found $DNS_COUNT orphaned DNS record(s):"
              aws route53 list-resource-record-sets \
                --hosted-zone-id "$HOSTED_ZONE_ID" \
                --query "ResourceRecordSets[?Name=='www.${{ env.DOMAIN }}.'].[Name,Type]" \
                --output table
            else
              echo "‚úÖ No orphaned Route53 records"
            fi
          else
            echo "‚ö†Ô∏è Hosted zone not found or error occurred"
          fi
          
          echo ""
          TOTAL_ORPHANS=$((ALB_COUNT + TG_COUNT + SG_COUNT + DNS_COUNT))
          if [ "$TOTAL_ORPHANS" -gt "0" ]; then
            echo "‚ùå Cleanup verification FAILED - found $TOTAL_ORPHANS orphaned resource(s)"
            echo "‚ö†Ô∏è You may need to run manual cleanup"
            exit 1
          else
            echo "‚úÖ Cleanup verification PASSED - no orphaned resources found!"
          fi


  destroy-bootstrap:
    needs: deploy
    if: github.event.inputs.action == 'destroy' && success()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infra
        run: |
          cd infra
          terraform destroy -auto-approve